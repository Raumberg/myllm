[model]
model_name_or_path = "yandex/YandexGPT-5-Lite-8B-pretrain"

[datasets]
dataset = "d0rj/gsm8k-ru"
problem_field = "question"
solution_field = "answer"
dataloader_num_workers = 2
test_size = 0.1
extract_hash = true

[run]
run_name = "rl-ygpt-8"
report_to = "wandb"
logging_first_step = true
logging_steps = 1
save_strategy = "steps"
save_steps = 500
save_total_limit = 5
output_dir = "models/attn-signs-ygpt-8"
project_name = "rl-ygpt"

[training]
num_train_epochs = 1
per_device_train_batch_size = 2
learning_rate = 0.00005
bf16 = true
seed = 42
use_peft = true

[grpo]
use_vllm = true
# vllm_server_host = "172.23.73.225"
# vllm_server_port = "8000"
num_generations = 2
max_completion_length = 2048
num_iterations = 1          # https://github.com/huggingface/trl/releases/tag/v0.16.0
scale_rewards = false       # should be default var
beta = 0.04                 # reference model beta in vllm
epsilon_high = 0.28         # Increasing upper bound epsilon leads to higher entropy during generation, promoting better exploration
preload_rm = false

[lora]
lora_target_modules = [
    "k_proj",
    "v_proj",
    "q_proj",
    "o_proj",
    "gate_proj",
    "up_proj",
    "down_proj",
]
lora_r = 32
lora_alpha = 64

[fusion]
use_liger = false
attn_implementation = "flash_attention_2"

[tokenizer]
eos_token =  "</s>"
pad_token = "[SPEC_TOKEN_1001]"
chat_template = "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<s>' + message['role'] + '\n' + message['content'] + '</s>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<s>assistant\n' }}{% endif %}"
force_chat_template = true
system_prompt = """
[MODE: Deep Thinking]
"""
