[model]
model_name_or_path = "/home/nshestopalov/projects/myllm/models/gptr-8"

[datasets]
dataset = "attn-signs/gromov-3"
problem_field = "problem"
solution_field = "answer"
dataloader_num_workers = 2
test_size = 0.1
extract_hash = false

[run]
run_name = "rl-gptr-8-stage3"
report_to = "wandb"
logging_first_step = true
logging_steps = 1
save_strategy = "steps"
save_steps = 500
save_total_limit = 5
output_dir = "models/attn-signs-gptr-8-grpo-stage3"
project_name = "rl-gptr"

[training]
num_train_epochs = 1
per_device_train_batch_size = 3
learning_rate = 0.00003
bf16 = true
seed = 42
use_peft = true

[grpo]
use_vllm = true
num_generations = 3
max_completion_length = 2048
num_iterations = 1          # https://github.com/huggingface/trl/releases/tag/v0.16.0
scale_rewards = false       # should be default var
beta = 0.00                  # reference model beta in vllm
epsilon_high = 0.29         # Increasing upper bound epsilon leads to higher entropy during generation, promoting better exploration
preload_rm = false
reflection_prompt = """
Перед ответом проведи внутренний диалог и проанализируй проблему. 
ОБЯЗАТЕЛЬНО используй этот формат:

<think>
твои размышления
</think>
**Окончательный ответ**
твой ответ
"""
reflection_chance = 0.3

[lora]
lora_target_modules = [
    "k_proj",
    "v_proj",
    "q_proj",
    "o_proj",
    "gate_proj",
    "up_proj",
    "down_proj",
]
lora_modules_to_save = ["embed_tokens"]
lora_r = 64
lora_alpha = 128

[fusion]
use_liger = true
attn_implementation = "flash_attention_2"

[tokenizer]
eos_token =  "</s>"
pad_token = "<unk>"
chat_template = "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<s>' + message['role'] + '\n' + message['content'] + '</s>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<s>assistant\n<think>\n' }}{% endif %}"
force_chat_template = true
added_special_tokens = [
    "<think>",
    "</think>"
]
system_prompt = """
[MODE: Reflection]
"""

