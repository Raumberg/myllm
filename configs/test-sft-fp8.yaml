model:
  name: attn-signs/GPTR-8b-v2
  dtype: bf16
  use_peft: false
  
  # WARNING: Use only on Hopper series GPUs, FP8 is only supported on SM90 (H100) and newer GPUs
  # May not work now (still in 'beta' stage)
  # casting to FP8 using transformer engine
  cast_to_fp8: true

  # Attention optimisation backend (optional): xformers | flash_attention_2
  # attn_implementation: sage_attention : Flash Attention 2 is not supported with FP8

wandb:
  enable: true
  project: myllm-experiments
  name: gptr8b-sft-run1

training:
  micro_batch_size: 1
  gradient_accumulation_steps: 8
  epochs: 1
  lr: 0.00006
  warmup_steps: 10
  gradient_checkpointing: true
  max_seq_length: 2048
  logging_steps: 1
  use_liger_kernel: false # TODO: check if it works with FP8

logging:
  level: info
  suppress: ["transformers", "datasets", "deepspeed"]
  warnings_ignore: ["use_cache=True", "TORCH_CUDA_ARCH_LIST"]
  disable_tqdm: true

engine:
  name: deepspeed
  config: configs/deepspeed/stage_3_fp8.json

dataset:
  name: attn-signs/kolmogorov-3
  split: train[:5%]
  test_size: 0.01        # will derive eval split automatically
  max_length: 2048

  # Advanced processing
  processor_type: default     # default | history | grpo | pair
  text_field: conversation
  system_prompt: "[MODE: Reflection]"
  model_support_system_role: true

  # misc
  offline: false                  # set true to forbid HF downloads
  chat_template: null             # provide custom template string if needed

collator:
  type: completion
  template: "<s>assistant" 
  ignore_index: -100
  verbose: true