model:
  name: attn-signs/GPTR-8b-v2
  dtype: bf16
  use_peft: true
  lora_r: 16
  lora_alpha: 32
  lora_dropout: 0.05
  lora_target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]
  lora_task_type: "CAUSAL_LM"

  # Quantisation â€“ either 4bit or 8bit (mutually exclusive)
  use_4bit: false
  use_8bit: false  # load in 8-bit via bitsandbytes
  bnb_compute_dtype: "bf16"  # compute dtype for 4-bit if enabled

  # Attention optimisation backend (optional): xformers | flash_attention_2
  attn_implementation: flash_attention_2

wandb:
  enable: true
  project: myllm-experiments
  name: gptr8b-sft-run1

training:
  micro_batch_size: 1
  gradient_accumulation_steps: 8
  epochs: 1
  lr: 0.00006
  warmup_steps: 10
  gradient_checkpointing: true
  max_seq_length: 8192
  logging_steps: 1
  use_liger_kernel: true

logging:
  level: info
  suppress: ["transformers", "datasets", "deepspeed"]
  warnings_ignore: ["use_cache=True", "TORCH_CUDA_ARCH_LIST"]
  disable_tqdm: true

engine:
  name: deepspeed
  config: configs/deepspeed/stage_3.json

dataset:
  # name: attn-signs/gromov-max
  name: attn-signs/gromov-max
  split: train[:5%]
  test_size: 0.01        # will derive eval split automatically
  max_length: 8192

  # Advanced processing
  processor_type: pair     # default | history | grpo | pair
  problem_field: problem
  answer_field: solution_1
  # processor_type: default  # can be default ('conversation' column will be used to construct chat)
  # text_field: conversation
  system_prompt: "[MODE: Reflection]"
  model_support_system_role: true

  # misc
  offline: false                  # set true to forbid HF downloads
  chat_template: null             # provide custom template string if needed

collator:
  type: completion
  template: "<s>assistant" 
  ignore_index: -100
  verbose: true