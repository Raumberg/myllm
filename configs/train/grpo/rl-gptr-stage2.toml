[model]
model_name_or_path = "/home/nshestopalov/projects/myllm/models/gptr-8"

[datasets]
dataset = "attn-signs/gromov-1"
problem_field = "problem"
solution_field = "answer"
dataloader_num_workers = 2
test_size = 0.1
extract_hash = false

[run]
run_name = "rl-gptr-8-stage2"
report_to = "wandb"
logging_first_step = true
logging_steps = 1
save_strategy = "steps"
save_steps = 500
save_total_limit = 5
output_dir = "models/attn-signs-gptr-8-grpo-stage2"
project_name = "rl-gptr"

[training]
num_train_epochs = 1
per_device_train_batch_size = 2
learning_rate = 0.00003
bf16 = true
seed = 42
use_peft = true

[grpo]
use_vllm = true
num_generations = 2
max_completion_length = 2048
num_iterations = 1          # https://github.com/huggingface/trl/releases/tag/v0.16.0
scale_rewards = false       # should be default var
beta = 0.04                 # reference model beta in vllm
epsilon_high = 0.28         # Increasing upper bound epsilon leads to higher entropy during generation, promoting better exploration
preload_rm = false
reflection_prompt = """
Перед ответом проведи внутренний диалог. Обязательно:
1. Рассмотри минимум 2 альтернативных подхода
2. Упомяни возможные слабые места своих рассуждений
3. Используй маркеры неопределенности

Пример структуры:
<think>
[Вопрос] Стоит ли...?
[Гипотеза 1] Возможно... Однако...
[Гипотеза 2] Альтернативно... Но...
[Сомнения] Не уверен в... Может быть...
[Заключение] Таким образом...
</think>
**Окончательный ответ**
твой ответ
"""
reflection_chance = 0.2

[lora]
lora_target_modules = [
    "k_proj",
    "v_proj",
    "q_proj",
    "o_proj",
    "gate_proj",
    "up_proj",
    "down_proj",
]
lora_modules_to_save = ["embed_tokens"]
lora_r = 64
lora_alpha = 128

[fusion]
use_liger = true
attn_implementation = "flash_attention_2"

[tokenizer]
eos_token =  "</s>"
pad_token = "<unk>"
chat_template = "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<s>' + message['role'] + '\n' + message['content'] + '</s>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<s>assistant\n<think>\n' }}{% endif %}"
force_chat_template = true
added_special_tokens = [
    "<think>",
    "</think>"
]
system_prompt = """
[MODE: Reflection]
"""

